{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2629a7ee",
   "metadata": {},
   "source": [
    "# Predicting Interview No-shows\n",
    "\n",
    "**Problem statement:**\n",
    "\n",
    "Your company, Acme Co., sources candidates for companies hiring new employees. Recently, a number of our clients have complained that candidates have not been showing up to interviews. Your boss has provided you with the attached data set in hopes that you can find some way of identifying candidates at risk of not attending scheduled interviews\n",
    "\n",
    "Logistic regression or a tree-based method. It seems like there are a good amount of categorical features, so may be learning towards trees.\n",
    "\n",
    "**Import Libraries:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "f7beeccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "import re\n",
    "import pickle\n",
    "import googlemaps \n",
    "from datetime import datetime\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c27e33",
   "metadata": {},
   "source": [
    "**Functions:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "91e034b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_special_characters(string):\n",
    "    l = re.sub('[^A-Za-z0-9]+', ' ', string)\n",
    "    return l.strip()\n",
    "\n",
    "def get_city_diff(city_1,city_2):\n",
    "    distance = gmaps.distance_matrix(city_1,city_2)['rows'][0]['elements'][0]\n",
    "    return distance['distance']['value']\n",
    "\n",
    "def replace_value(val,replacable_words_li,replace_word):\n",
    "    if val in replacable_words_li:\n",
    "        return replace_word\n",
    "    else: \n",
    "        return val\n",
    "    \n",
    "def get_date_string(date):\n",
    "    val = re.search(\"^(\\d)+[\\.\\-\\/](\\d)+[\\.\\-\\/](\\d)*\",date).group()\n",
    "    val = val.replace(\"/\",'.').replace(\"-\",\".\")\n",
    "    d,m = val.split('.')[0].zfill(2),val.split('.')[1].zfill(2)\n",
    "    if len(val.split('.')[2]) == 4:\n",
    "        yr = val.split('.')[2]\n",
    "    else:\n",
    "        yr = \"20\".join(val.split('.')[2])\n",
    "    return \".\".join([d,m,yr])\n",
    "    \n",
    "def clean_date_formating(row):\n",
    "    try:\n",
    "        date = get_date_string(row)\n",
    "        return datetime.strptime(date, '%d.%m.%Y')\n",
    "    except:\n",
    "        date = '.'.join([x[:2],'04.2016'])\n",
    "        return datetime.strptime(date, '%d.%m.%Y')\n",
    "        \n",
    "    \n",
    "def format_date(row):\n",
    "    r = clean_date_formating(row)\n",
    "    \n",
    "    if int(r.split('.')[0]) <= 12 and int(r.split('.')[1]) <= 12:\n",
    "        return \"both_under\"\n",
    "    elif int(r.split('.')[0]) > 12:\n",
    "        return \"d-m-y\"\n",
    "    elif int(r.split('.')[1]) > 12: \n",
    "        return \"m-d-y\"\n",
    "    else:\n",
    "        return \"other\"\n",
    "    \n",
    "def clean_locations(locale):\n",
    "    return locale.replace(\"-\",'').split('/')[0].strip().replace(\"-\",'')\n",
    "\n",
    "def clean_col_vals(row,value_replacements):\n",
    "    if row in value_replacements.keys():\n",
    "        return value_replacements[row].replace(' ','_')\n",
    "    else:\n",
    "        return row.replace(' ','_')\n",
    "    \n",
    "def clean_position_names(row):\n",
    "    return row.replace('-','').replace(' ','_')\n",
    "\n",
    "def replace_time_with_position(skills,position):\n",
    "    try:\n",
    "        re.search(\"(\\d)+[\\.](\\d{2}\\s)[\\w]{2}\",skills).group()\n",
    "        return position\n",
    "    except:\n",
    "        return skills\n",
    "    \n",
    "def lead_or_manager_skills_fix(skills,position,leads,position_or_lead=\"position\"):\n",
    "    ptrn = \"[\\_]?(senior|manager|lead|sr|tech_lead|technical_lead)[\\_]?\"\n",
    "    try:\n",
    "        s = re.search(ptrn,skills).group()\n",
    "        if (s == skills) and position_or_lead==\"position\":\n",
    "            return position\n",
    "        elif position_or_lead!=\"position\":\n",
    "            return 1\n",
    "        else:\n",
    "            return skills.replace(s,'')\n",
    "    \n",
    "    except:\n",
    "        if position_or_lead==\"position\":\n",
    "            return skills\n",
    "        else: \n",
    "            return 0\n",
    "\n",
    "def clean_col_vals_new(row,value_replacements):\n",
    "    if row == \"basesas program/ reporting\":\n",
    "        return \"basesas_program_reporting\"\n",
    "    for val in value_replacements.keys():\n",
    "        if row in value_replacements[val]:\n",
    "            val.split(',')\n",
    "            v=val.strip().replace('-',' ')\n",
    "            s = re.sub(' +', ' ', v)\n",
    "            return s.replace(' ','_')\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    if row.find(',') != -1:\n",
    "        row = '/'.join([x.strip() for x in row.split(',')])\n",
    "    \n",
    "    v = row.replace('-',' ').replace(\"–\",\" \").strip()\n",
    "    s = re.sub(' +', ' ', v) \n",
    "    return s.replace(' ','_')\n",
    "\n",
    "def value_contained_skill(skill,val):\n",
    "    for v in val:\n",
    "        if skill.find(v) != -1:\n",
    "            return 1\n",
    "        else: \n",
    "            return 0\n",
    "    \n",
    "def clean_skills(skills):\n",
    "    for s in ['/',',','–','-','(',')']:\n",
    "        skills = '_'.join([i.strip() for i in skills.split(s) if i != ''])\n",
    "\n",
    "    return skills.lower().replace('&',' and ').replace(' ','_').replace(\"__\",\"_\")\n",
    "\n",
    "def replace_skills(skill,replacements):\n",
    "    for key in skill_replacements.keys():\n",
    "        if skill in skill_replacements[key]:\n",
    "            return key\n",
    "        else: \n",
    "            pass\n",
    "        \n",
    "    return skill\n",
    "\n",
    "def clean(val):\n",
    "    return val.replace(\", India\",\"\")\n",
    "\n",
    "def get_distance(city_1,city_2):\n",
    "    try:\n",
    "        if city_1 == city_2:\n",
    "            return 0\n",
    "        elif city_1[1] > city_2[1]:\n",
    "            return updated_distances[city_1][city_2]\n",
    "        elif city_1[1] < city_2[1]:\n",
    "            return updated_distances[city_2][city_1]\n",
    "        else: \n",
    "            return updated_distances[city_1][city_2]\n",
    "    except:\n",
    "        return updated_distances[city_2][city_1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aad18e3",
   "metadata": {},
   "source": [
    "Read in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "807cdb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Interview_Input.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba8a53b",
   "metadata": {},
   "source": [
    "### Data Cleaning:\n",
    "\n",
    "We now need to look for missing values, data types, correlations, etc. First let's deal with missing values.\n",
    "\n",
    "Drop read in empty columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "dcd7c0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['Unnamed: 22','Unnamed: 23','Unnamed: 24','Unnamed: 25','Unnamed: 26'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "dc6a128a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31020641",
   "metadata": {},
   "source": [
    "for the missing date item, I may just drop. Once checking it (show below), we can see that the entire row is missing, so this one will be dropped:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d4f4b422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date of Interview</th>\n",
       "      <th>Client name</th>\n",
       "      <th>Industry</th>\n",
       "      <th>Location</th>\n",
       "      <th>Position to be closed</th>\n",
       "      <th>Nature of Skillset</th>\n",
       "      <th>Interview Type</th>\n",
       "      <th>Name(Cand ID)</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Candidate Current Location</th>\n",
       "      <th>...</th>\n",
       "      <th>Candidate Native location</th>\n",
       "      <th>Have you obtained the necessary permission to start at the required time</th>\n",
       "      <th>Hope there will be no unscheduled meetings</th>\n",
       "      <th>Can I Call you three hours before the interview and follow up on your attendance for the interview</th>\n",
       "      <th>Can I have an alternative number/ desk number. I assure you that I will not trouble you too much</th>\n",
       "      <th>Have you taken a printout of your updated resume. Have you read the JD and understood the same</th>\n",
       "      <th>Are you clear with the venue details and the landmark.</th>\n",
       "      <th>Has the call letter been shared</th>\n",
       "      <th>Observed Attendance</th>\n",
       "      <th>Marital Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1233</th>\n",
       "      <td>NaN</td>\n",
       "      <td>﻿﻿</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Date of Interview Client name Industry Location Position to be closed  \\\n",
       "1233               NaN          ﻿﻿      NaN      NaN                   NaN   \n",
       "\n",
       "     Nature of Skillset Interview Type Name(Cand ID) Gender  \\\n",
       "1233                NaN            NaN           NaN    NaN   \n",
       "\n",
       "     Candidate Current Location  ... Candidate Native location  \\\n",
       "1233                        NaN  ...                       NaN   \n",
       "\n",
       "     Have you obtained the necessary permission to start at the required time  \\\n",
       "1233                                                NaN                         \n",
       "\n",
       "     Hope there will be no unscheduled meetings  \\\n",
       "1233                                        NaN   \n",
       "\n",
       "     Can I Call you three hours before the interview and follow up on your attendance for the interview  \\\n",
       "1233                                                NaN                                                   \n",
       "\n",
       "     Can I have an alternative number/ desk number. I assure you that I will not trouble you too much  \\\n",
       "1233                                                NaN                                                 \n",
       "\n",
       "     Have you taken a printout of your updated resume. Have you read the JD and understood the same  \\\n",
       "1233                                                NaN                                               \n",
       "\n",
       "     Are you clear with the venue details and the landmark.  \\\n",
       "1233                                                NaN       \n",
       "\n",
       "     Has the call letter been shared Observed Attendance Marital Status  \n",
       "1233                             NaN                 NaN            NaN  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['Date of Interview'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "52b1f089",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(subset=[\"Date of Interview\"],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df61f398",
   "metadata": {},
   "source": [
    "Ok, lets look at the values in these binary columns. There seems to be some issues with input values. The values need to be cleaned. So, let's look into the column values to clean them proplerly. First, let's lower all of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "1581d78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in data.columns:\n",
    "    data[col] = data[col].str.lower()\n",
    "    data[col] = data[col].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6199f67",
   "metadata": {},
   "source": [
    "I'm making a dictionary with all columns values, except for dates and names: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "1d2986b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_vals_2 = {}\n",
    "\n",
    "for col in data.columns:\n",
    "    if col in [\"Date of Interview\",\"Name(Cand ID)\"]:\n",
    "        pass\n",
    "    else:\n",
    "        col_vals_2[col] = data[col].value_counts().keys().to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e83380",
   "metadata": {},
   "source": [
    "Update values for the question based answers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "340a1ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_for_maybe = ['Has the call letter been shared','Hope there will be no unscheduled meetings']\n",
    "    \n",
    "cols_for_YN = ['Are you clear with the venue details and the landmark.',\n",
    "               'Have you taken a printout of your updated resume. Have you read the JD and understood the same',\n",
    "               'Can I have an alternative number/ desk number. I assure you that I will not trouble you too much',\n",
    "               'Can I Call you three hours before the interview and follow up on your attendance for the interview',\n",
    "               'Have you obtained the necessary permission to start at the required time']\n",
    "\n",
    "vals_for_maybe_dict = ['need to check','not yet','havent checked','yet to check','cant say']\n",
    "\n",
    "vals_for_YN = ['no- i need to check','not yet','no- will take it soon','no',\n",
    "               'no i have only thi number','no','no dont','not yet','yet to confirm']\n",
    "\n",
    "for col in cols_for_maybe:\n",
    "    data[col] = data[col].apply(lambda val: replace_value(val,vals_for_maybe_dict,\"not sure\"))\n",
    "    \n",
    "for col in cols_for_YN:\n",
    "    data[col] = data[col].apply(lambda val: replace_value(val,vals_for_YN,\"no\"))\n",
    "    \n",
    "#replace_value(val,replacable_words_li,replace_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c45614b",
   "metadata": {},
   "source": [
    "There is a string value that is \"na\". In a way to understand if it is meant to be read as \"N/A\" or an accident for \"no\", I've pulled the data where this value shows up. \n",
    "\n",
    "For most of the columns, it is showing up for the 20 same rows (found using `data[data['Hope there will be no unscheduled meetings']=='na']`). I looked to see what is common about these rows to determine if there are any commonaility that would make it sensible to be N/A or if it should be no. However, I think based on the commonaility across the board, I'm leaning to it falling into the NaN category. I will most likely assign it as such, and deal with null values after one-hot encoding.\n",
    "\n",
    "I now need to address the formatting issues with the date column. Some values are separated differently. A few list date and time, while others start with months as the first value. Below we will figure out how we can manage these and fix them.\n",
    "\n",
    "was used before but isn't now:\n",
    "\n",
    "`data[\"date_formats\"] = data[\"Date of Interview\"].apply(lambda row: format_date(row))`\n",
    "`data[\"date_formats\"].value_counts()`\n",
    "\n",
    "based on this, I am assuming a format of \"dd/mm/yyyy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "4550e179",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Date of Interview\"] = data[\"Date of Interview\"].apply(lambda row: clean_date_formating(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2b84bd",
   "metadata": {},
   "source": [
    "Based on the fact this model is forward looking, I'm going to **not going to use the year** in training our model. We will have a separate month and day column. But we will do this after cleaning the rest of the data.\n",
    "\n",
    "Next clean all location values: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "c940a6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['Location','Candidate Current Location','Candidate Job Location',\n",
    "            'Interview Venue','Candidate Native location']:\n",
    "    data[col] = data[col].apply(lambda loc: clean_locations(loc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a95ca7",
   "metadata": {},
   "source": [
    "fix values that need to be replaced:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "9227dd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "industry_replacements = {'it products and services':'it','it services':'it'}\n",
    "interview_type_replacements = {'scheduled walk in':'scheduled_walkin','scheduled walkin':'scheduled_walkin',\n",
    "                              'sceduled walkin':'scheduled_walkin'}\n",
    "client_replacements = {\"aon hewitt gurgaon\":\"aon\",\"aon hewitt\":\"aon\",\"hewitt\":\"aon\",\n",
    "                       \"standard chartered bank chennai\":\"standard chartered bank\"}\n",
    "\n",
    "data['Industry'] = data['Industry'].apply(lambda row: clean_col_vals(row,industry_replacements))\n",
    "data['Interview Type'] = data['Interview Type'].apply(lambda row: clean_col_vals(row,interview_type_replacements))\n",
    "data[\"Position to be closed\"] = data[\"Position to be closed\"].apply(lambda row: clean_position_names(row))\n",
    "data['Client name'] = data['Client name'].apply(lambda row: clean_col_vals(row,client_replacements))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be87ccac",
   "metadata": {},
   "source": [
    "Look into skillsets to determine how we can make more out of this data. There's a lot of data in each cell, we can break them down more to better use the data.\n",
    "\n",
    "first thing, if there is a time in the skillset just put the `Position to be closed` value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "ce70e9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Nature of Skillset\"] = data.apply(lambda row: replace_time_with_position(row[\"Nature of Skillset\"],row[\"Position to be closed\"]),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d164049f",
   "metadata": {},
   "source": [
    "Clean up some of these skills:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "a4e9975f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"skillset\"] = data['Nature of Skillset'].apply(lambda row: clean_skills(row))\n",
    "data[\"is_lead_or_manager\"] = data.apply(lambda row: lead_or_manager_skills_fix(row['Nature of Skillset'],row['Position to be closed'],leads_li,position_or_lead=\"lead\"),axis=1)\n",
    "data[\"skillset\"] = data.apply(lambda row: lead_or_manager_skills_fix(row[\"skillset\"],row['Position to be closed'],leads_li,position_or_lead=\"position\"),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8e12b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_replacements = {\"java_j2ee_struts_hibernate_spring_jsf_xml\":[\"java_j2ee_struts_hibernate\",\"java_spring_hibernate_jsf\",\n",
    "                        \"java_j2ee\",\"java_spring_hibernate\",\"java_j2ee_core_java\",\"java_jsf\",\"java_j2ee_jsf\",\n",
    "                        \"java_xml_struts_hibernate\"],\n",
    "                        \"java_developer\":[\"java\",\"core_java\",\"java_sql\"],\n",
    "                        \"cdd_kyc\":[\"aml_kyc_cdd\"],\n",
    "                        \"lending_and_liabilities\":[\"l_and_l\",\"lending_and_liablities\"],\n",
    "                        \"biosimillar\":[\"biosimilars\"],\n",
    "                        \"oracle\":[\"oracle_plsql\"],\n",
    "                        \"label\":[\"ra_label\",\"global_labelling\"],\"cots\":[\"cots_developer\"]\n",
    "                     }\n",
    "    \n",
    "tester = data['skillset'].apply(lambda row: replace_skills(row,skill_replacements))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a01729",
   "metadata": {},
   "source": [
    "get posts that are for manager,senior, or lead positions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "b31ec1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_replacements = {\"lending_and_liabilities\":[\"l_and_l\",\"lending_and_liability\"],\n",
    "                      \"biosimillar\":[\"biosimilars\"],\n",
    "                      \"etl\":[\"tl\"]\n",
    "                     }\n",
    "    \n",
    "data['skillset'] = data['skillset'].apply(lambda row: replace_skills(row,skill_replacements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "b58c8781",
   "metadata": {},
   "outputs": [],
   "source": [
    "role_values = {\"java\":[\"java\"],\"j2ee\":[\"j2ee\"],\"hibernate\":[\"hibernate\"],\n",
    "               \"jsf\":[\"jsf\"],\"spring\":[\"spring\"],\"sql\":[\"sql\",\"ms_exchange\"],\n",
    "               \"sas\":[\"sas\"],\"sccm\":[\"sccm\"],\"aml\":[\"aml\"],\"ra\":[\"ra\"],\n",
    "               \"developer\":[\"developer\",\"software_engineer\"],\"testing\":[\"testing\"],\n",
    "               \"production\":[\"production\"],\"mednet\":[\"mednet\"],\"operations\":[\"operations\"],\n",
    "               \"cots\":[\"cots\"],\"oracle\":[\"oracle\"],\"analytics\":[\"analy\"],\"regulatory\":[\"regulatory\"],\n",
    "               \"hadoop\":[\"hadoop\"],\"automation\":[\"automation\"],\"label\":[\"label\"],\"fresher\":[\"fresher\"],\n",
    "               \"cdd_kyc\":[\"cdd_kyc\"],\"dot_net\":[\"dot_net\"],\"emea\":[\"emea\"],\"routine\":[\"routine\"],\n",
    "               \"lending_and_liabilities\":[\"lending_and_liabilities\"],\"biosimillar\":[\"biosimillar\"],\n",
    "               \"etl\":[\"etl\"],\"publishing\":[\"publishing\"],\"#name?\":[\"#name?\"],\n",
    "                 \"support\":[\"support\"],\"network\":[\"network\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf505a0",
   "metadata": {},
   "source": [
    "let's get all of the values to make individual columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "2fc9fd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "for val in role_values.keys():  \n",
    "    col_name = f\"{val}_role\"\n",
    "    data[col_name] = data.skillset.apply(lambda skill: value_contained_skill(skill,role_values[val]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f4cae1",
   "metadata": {},
   "source": [
    "ok, now to fill in null values.\n",
    "\n",
    "The \"na\" value in pandas are read already as null values. no need to run the following to impute them to nan:\n",
    "\n",
    "```\n",
    "cols = ['Are you clear with the venue details and the landmark.',\n",
    "        'Have you taken a printout of your updated resume. Have you read the JD and understood the same',\n",
    "        'Can I have an alternative number/ desk number. I assure you that I will not trouble you too much',\n",
    "        'Can I Call you three hours before the interview and follow up on your attendance for the interview',\n",
    "        'Have you obtained the necessary permission to start at the required time',\n",
    "        'Has the call letter been shared','Hope there will be no unscheduled meetings']\n",
    "\n",
    "for i in cols:\n",
    "    data[i] = data[i].replace(to_replace='na', value=None)\n",
    "    print(data[i].unique())`\n",
    "```\n",
    "\n",
    "for simplicities sake of this challenge I will be filling the missing values with the mode values. XGBoost is an option to use in a case like this as well, but to be more intreprtable, we're going to continue with this methid.\n",
    "\n",
    "I will be removing the row from the DataFrame that do not have values in the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "6ed15d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_dep = data[data['Observed Attendance'].isna()]\n",
    "df = data[~data['Observed Attendance'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "496fd961",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = df.isnull().sum()\n",
    "l = pd.DataFrame(l)\n",
    "na_columns = l[l[0] > 0].index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "b13e3f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in na_columns:\n",
    "    val_to_fill = data[col].mode()[0]\n",
    "    df[col] = df[col].fillna(val_to_fill)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145a2753",
   "metadata": {},
   "source": [
    "I'm going to drop the `Name(Cand ID)` it doesn't offer any details about the candidate really.\n",
    "\n",
    "Other columns I will be dropping here are: `Nature of Skillset`,`Location`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "9cf760a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"Name(Cand ID)\",\"Nature of Skillset\",\"Location\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29f2583",
   "metadata": {},
   "source": [
    "get month and day extracted from `Date of Interview`, then drop the column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "d0e40e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"month\"] = df[\"Date of Interview\"].apply(lambda date: date.month)\n",
    "df[\"day\"] = df[\"Date of Interview\"].apply(lambda date: date.day)\n",
    "df[\"day_of_year\"] = df[\"Date of Interview\"].apply(lambda date: date.timetuple().tm_yday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "636e3e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"Date of Interview\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2665358a",
   "metadata": {},
   "source": [
    "Now to deal with options for locations so we can remove some of these categorical values. We will use the google api to get values for the following:\n",
    "    \n",
    "distance between:\n",
    "- `Candidate Current Location` from `Candidate Job Location`\n",
    "- `Candidate Current Location` from `Interview Venue`\n",
    "- `Candidate Current Location` from `Candidate Native location`\n",
    "- `Candidate Job Location` from `Interview Venue`\n",
    "- `Candidate Job Location` from `Candidate Native location`\n",
    "- `Interview Venue` from `Candidate Native location`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "ae5cf693",
   "metadata": {},
   "outputs": [],
   "source": [
    "distaince_to_check = [('Candidate Current Location','Candidate Job Location'),\n",
    "                      ('Candidate Current Location','Interview Venue'),\n",
    "                      ('Candidate Current Location','Candidate Native location'),\n",
    "                      ('Candidate Job Location','Interview Venue'),\n",
    "                      ('Candidate Job Location','Candidate Native location'),\n",
    "                      ('Interview Venue','Candidate Native location')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "103810e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../api_key.pickle', 'rb') as handle:\n",
    "    api_info = pickle.load(handle)\n",
    "\n",
    "gmaps = googlemaps.Client(key=api_info[\"api_key\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67324cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_cols = [\"Candidate Current Location\",\"Candidate Job Location\",\"Interview Venue\",\"Candidate Native location\"]\n",
    "\n",
    "locs = []\n",
    "for col in loc_cols:\n",
    "    locs.extend(df[col].unique())\n",
    "\n",
    "locs = list(set(locs))\n",
    "locs = [f\"{city}, India\" for city in locs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "5412cdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = {}\n",
    "\n",
    "for c in itertools.combinations(locs, 2):\n",
    "    distances[c] = get_city_diff(c[0],c[1])\n",
    "    \n",
    "updated_distances = {}\n",
    "\n",
    "for i in distances:\n",
    "    one,two = clean(i[0]),clean(i[1])\n",
    "    if one[1] > two[1]:\n",
    "        try:\n",
    "            updated_distances[one][two] = distances[i]\n",
    "        except:\n",
    "            updated_distances[one] = {}\n",
    "            updated_distances[one][two] = distances[i]\n",
    "    else: \n",
    "        try:\n",
    "            updated_distances[two][one] = distances[i]\n",
    "        except:\n",
    "            updated_distances[two] = {}\n",
    "            updated_distances[two][one] = distances[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698e310d",
   "metadata": {},
   "source": [
    "Now that we have all the differences, we are going to use them to fill in these four distance columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "da459684",
   "metadata": {},
   "outputs": [],
   "source": [
    "distaince_to_check = [('Candidate Current Location','Candidate Job Location'),\n",
    "                      ('Candidate Current Location','Interview Venue'),\n",
    "                      ('Candidate Current Location','Candidate Native location'),\n",
    "                      ('Candidate Job Location','Interview Venue'),\n",
    "                      ('Candidate Job Location','Candidate Native location'),\n",
    "                      ('Interview Venue','Candidate Native location')]\n",
    "    \n",
    "for col in distaince_to_check:\n",
    "    new_col = ' to '.join(col).replace(' ','_')\n",
    "    df[new_col] = df.apply(lambda row:get_distance(row[col[0]],row[col[1]]),axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5bcf07",
   "metadata": {},
   "source": [
    "Now we need to one hot encode the categorical columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "f6c587d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.get_dummies(data,columns=[\"Have you obtained the necessary permission to start at the required time\",\n",
    "#                              \"Hope there will be no unscheduled meetings\",\n",
    "#                              \"Can I Call you three hours before the interview and follow up on your attendance for the interview\",\n",
    "#                              \"Can I have an alternative number/ desk number. I assure you that I will not trouble you too much\",\n",
    "#                              \"Have you taken a printout of your updated resume. Have you read the JD and understood the same\",\n",
    "#                              \"Are you clear with the venue details and the landmark.\",\n",
    "#                              \"Has the call letter been shared\",\n",
    "#                              \"Observed Attendance\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b297d893",
   "metadata": {},
   "source": [
    "### EDA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d410d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45613f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "regression-premium",
   "language": "python",
   "name": "regression-premium"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
